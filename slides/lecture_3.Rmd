---
title: Fitting logistic regression models
output:
  xaringan::moon_reader:
    css: "lab-slides.css"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

### Recap: Fisher scoring

.large[
$$Y_i \sim Bernoulli(p_i)$$

$$\log \left( \dfrac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_{i,1} + \cdots + \beta_k X_{i,k}$$

Choose $\beta = (\beta_0, ..., \beta_k)^T$ to maximize $L(\beta) = \prod \limits_{i=1}^n p_i^{Y_i} (1 - p_i)^{1 - Y_i}$.

How? **Fisher scoring:**
]

---

### Practice question: Fisher scoring

.large[
Suppose that $\log \left( \dfrac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_{i}$, and we have

$$\beta^{(r)} = \begin{bmatrix} -3.1 \\ 0.9 \end{bmatrix}, \hspace{1cm} U(\beta^{(r)}) = \begin{bmatrix} 9.16 \\ 31.91 \end{bmatrix},$$
$$\mathcal{I}(\beta^{(r)}) = \begin{bmatrix} 17.834 & 53.218 \\ 53.218 & 180.718 \end{bmatrix}$$

.question[
Use the Fisher scoring algorithm to calculate $\beta^{(r + 1)}$ (you may use R or a calculator, you do not need to do the matrix arithmetic by hand). Take $\sim 5$ minutes, then we will discuss.
]
]

---

### Alternative to Fisher scoring: gradient ascent

.large[
$$Y_i \sim Bernoulli(p_i)$$

$$\log \left( \dfrac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_{i,1} + \cdots + \beta_k X_{i,k}$$

Choose $\beta = (\beta_0, ..., \beta_k)^T$ to maximize $L(\beta)$.

**Gradient ascent:**
]

---

### Motivation for gradient ascent: walking uphill

---

### Practice question: gradient ascent

.large[
Suppose that $\log \left( \dfrac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_{i}$, and we have

$$\beta^{(r)} = \begin{bmatrix} -3.1 \\ 0.9 \end{bmatrix}, \hspace{1cm} U(\beta^{(r)}) = \begin{bmatrix} 9.16 \\ 31.91 \end{bmatrix}$$
.question[
* Use gradient ascent with a learning rate (aka step size) of $\gamma = 0.01$ to calculate $\beta^{(r+1)}$. 
* The actual maximum likelihood estimate is $\widehat{\beta} = (-3.360, 1.174)$. Does one iteration of gradient ascent or Fisher scoring get us closer to the optimal $\widehat{\beta}$?
* Discuss in pairs for 2--3 minutes.
]

]

---

### Gradient ascent vs. Fisher scoring

---

### Special topic: Feedforward neural networks

---

### Fitting neural networks: stochastic gradient descent

---

### Next steps

.large[
* Homework 1 released on course website
* Challenge assignments 1 and 2 released on course website
* Next week: logistic regression diagnostics
]

---

### Course components

.large[
* Regular homework assignments
  * Practice material from class
* Challenge assignments
  * Learn additional material related to course
* 2 take-home exams
  * Demonstrate knowledge of theory and methodology
  * No final exam!
* 2 projects
  * Apply material to real data and real research questions
]

---

### Grading philosophy

.large[
* Focusing on grades can detract from the learning process
* Homework should be an opportunity to *practice* the material. It is ok to make mistakes when practicing, as long as you make an honest effort
* Errors are a good opportunity to learn and revise your work
* Partial credit and weighted averages of scores make the meaning of a grade confusing. Does an 85 in the course mean you know 85% of everything, or everything about 85% of the material?
]

---

### Grading in this course

.large[
* I will give you feedback on every assignment
* Homework is graded on completeness and effort, not correctness
* All other assignments (challenge, exams, projects) are graded as Mastered / Not yet mastered
* If you haven't yet mastered something, you get to try again!
]

---

### Assigning grades: specifications grading

.large[
To get a **B** in the course:

* Receive credit for at least 5 homework assignments
* Master one project
* Master at least 80% of the questions on both exams

To get an **A** in the course:

* Receive credit for at least 5 homework assignments
* Master both projects
* Master at least 80% of the questions on both exams
* Master at least 2 challenge assignments
]

---

### Late work and resubmissions

.large[
* You get a bank of **5** extension days. You can use 1--2 days on any assignment, exam, or project.
* No other late work will be accepted (except in extenuating circumstances!)
* "Not yet mastered" challenge questions, exams, and projects may be resubmitted once
]